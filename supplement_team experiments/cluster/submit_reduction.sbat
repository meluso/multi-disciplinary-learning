#!/bin/bash -l

#-------------------  Begin SLURM preamble  -------------------------#
#SBATCH --job-name=islge_reductions_exec001
#SBATCH --partition=short
#SBATCH --ntasks=1
#SBATCH --nodes=1
#SBATCH --cpus-per-task=16
#SBATCH --mem=500GB
#SBATCH --time=0-03:00:00
#SBATCH --mail-type=ALL
#SBATCH --output=/gpfs1/home/j/m/%u/isl-group-experiments/logs/sets/%x_%j.log
#-------------------   End SLURM preamble   -------------------------#

# Echo cluster information
echo ""
echo "----- CLUSTER DETAILS -----"
echo ""
echo "  running host:    ${SLURMD_NODENAME}"
echo "  assigned nodes:  ${SLURM_JOB_NODELIST}"
echo "  partition used:  ${SLURM_JOB_PARTITION}"
echo "  jobid:           ${SLURM_JOBID}"
echo ""
echo "----- DIRECTORY & VARIABLE SETUP -----"
echo ""

# Specify directory
RUNDIR=/gpfs1/home/j/m/jmeluso/isl-group-experiments/src

# Move to model directory
cd $RUNDIR

echo "  Directory and variables setup successfully."
echo ""
echo "----- LOAD LIBRARIES -----"
echo ""

# Load python environment
source activate miniconda

echo "  Libraries loaded successfully."
echo ""
echo "----- BEGIN EXECUTIONS -----"
echo ""

# Run simulation in python for job ii
python run_reduction.py

echo ""
echo "  Results reduced successfully."
echo ""

# to submit 100 jobs, call the file submit_loop.sh
# to run live, use the following command:
#   srun --partition=bluemoon --ntasks=1 --mem=4G --time=4:00:00 --pty /bin/bash


